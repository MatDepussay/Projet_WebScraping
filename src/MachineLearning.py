# /// script
#    """
#       üìë Documentation : Pipeline de Machine Learning (Prix AutoScout24)
#       Ce script est le "cerveau" du projet. Il analyse les donn√©es de voitures d'occasion pour apprendre √† pr√©dire leur prix de vente.
#
#       üéØ Objectif
#       Transformer une base de donn√©es brute en un mod√®le capable d'estimer le prix d'un v√©hicule en fonction de ses caract√©ristiques (marque, kilom√©trage, puissance, etc.).
#
#       üõ†Ô∏è Les 5 √âtapes du Script
#           1. Pr√©paration et Clustering (Nettoyage final, Segmentation, Encodage)
#           2. Tuning (Optimisation de Random Forest et XGBoost)
#           3. Analyse d'Importance (Graphique)
#           4. √âvaluation et Comparaison (R¬≤, CV)
#           5. Export des R√©sultats (sauvegarde des mod√®les .pkl + fichiers Excel des erreurs)
#        
#       üì¶ Sorties du Script (Dossier /models)
#           - cluster_full_pipeline.pkl : Contient l'imputeur, le scaler et le mod√®le de clustering pour transformer les futures saisies utilisateur.
#           - best_rf_final.pkl	: Le mod√®le Random Forest entra√Æn√©.
#           - best_xgb_final.pkl : Le mod√®le XGBoost entra√Æn√©.
#           - model_features.pkl : La liste exacte des colonnes (indispensable pour l'App Streamlit).
#           - erreurs_rf_tuned.xlsx	: Liste des voitures o√π le mod√®le s'est tromp√©.
#    """
# requires-python = ">=3.12"
# dependencies = [
#     "polars",
#     "scikit-learn",
#     "xgboost",
#     "pandas",
#     "fastexcel",
#     "pyarrow",
#     "openpyxl",
#     "matplotlib",
#     "seaborn",
# ]
# ///

import polars as pl
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.experimental import enable_iterative_imputer # A NE SURTOUT PAS ENLEVER
from sklearn.impute import IterativeImputer, SimpleImputer


# =========================
# 1. CLUSTERING
# =========================
def ajouter_cluster_vehicule(df, n_clusters=5):
    """
    Cr√©e un clustering robuste des annonces via un Pipeline Scikit-Learn complet.
    
    Cette fonction automatise le pr√©traitement des donn√©es (imputation et mise √† l'√©chelle) 
    et l'application de l'algorithme K-Means. Elle g√®re intelligemment les donn√©es 
    manquantes pour √©viter de perdre des lignes pr√©cieuses.
    
    Logique d'Imputation :
    --------------------
    - Num√©rique (IterativeImputer) : Contrairement √† une imputation simple par la 
      moyenne, l'imputation it√©rative mod√©lise chaque colonne avec des valeurs 
      manquantes en fonction des autres colonnes. Elle "pr√©dit" par exemple la 
      'cylindree_l' en exploitant ses corr√©lations avec 'puissance_kw' et 'annee'.
    - Cat√©goriel (SimpleImputer) : Remplace les valeurs manquantes par la valeur 
      la plus fr√©quente (mode) du dataset.
    
    Parameters:
    -----------
    df : polars.DataFrame or pandas.DataFrame
        Le dataset contenant les annonces automobiles nettoy√©es.
    n_clusters : int, default=5
        Le nombre de groupes (segments de march√©) √† cr√©er.
    
    Workflow du Pipeline :
    ---------------------
    1. Pr√©traitement Num√©rique : Imputation it√©rative + Standardisation (Z-score).
    2. Pr√©traitement Cat√©goriel : Imputation par mode + One-Hot Encoding.
    3. Clustering : Application de K-Means sur les donn√©es transform√©es.
    4. Persistance : Sauvegarde du pipeline complet (incluant scaler et imputer) 
       pour une utilisation future en production.
    
    Returns:
    --------
    df : polars.DataFrame or pandas.DataFrame
        Le DataFrame original enrichi d'une colonne 'cluster_vehicule'.
    """
    print(f"üîç Clustering des v√©hicules ({n_clusters} clusters)...")
    
    # Conversion en Pandas pour sklearn (plus stable pour les pipelines complexes)
    data_pd = df.to_pandas() if isinstance(df, pl.DataFrame) else df.copy()

    num_features = ["annee", "kilometrage", "puissance_kw", "cylindree_l"]
    cat_features = ["carburant", "boite_de_vitesse", "transmission", "type_de_vehicule"]

    # 1. Pipeline num√©rique avec Imputation It√©rative (bas√©e sur la r√©gression)
    # L'imputer va apprendre les corr√©lations entre puissance, ann√©e et cylindr√©e
    num_pipeline = Pipeline([
        ('imputer', IterativeImputer(max_iter=10, random_state=42)),
        ('scaler', StandardScaler())
    ])

    # 2. Pipeline cat√©goriel (on garde most_frequent car l'imputation it√©rative est complexe sur le texte)
    cat_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', num_pipeline, num_features),
            ('cat', cat_pipeline, cat_features)
        ])

    cluster_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('kmeans', KMeans(n_clusters=n_clusters, random_state=42, n_init=10))
    ])

    # --- Calcul et Int√©gration ---
    clusters = cluster_pipeline.fit_predict(data_pd)
    
    # On ajoute la colonne √† data_pd (essentiel pour ton print final !)
    data_pd["cluster_vehicule"] = clusters
    
    # On met √† jour l'objet d'origine (df)
    if isinstance(df, pl.DataFrame):
        df = df.with_columns(pl.Series("cluster_vehicule", clusters))
    else:
        df["cluster_vehicule"] = clusters

    # Sauvegarde du pipeline COMPLET (mieux que 2 fichiers s√©par√©s)
    Path("models").mkdir(parents=True, exist_ok=True)
    with open("models/cluster_full_pipeline.pkl", "wb") as f:
        pickle.dump(cluster_pipeline, f)
    
    print("\nüìä R√©partition des clusters :")
    print(df['cluster_vehicule'].value_counts())
    
    # --- Analyse du Cluster 4 ---

    cluster_4_cars = data_pd[data_pd["cluster_vehicule"] == 4]
    print("\nüîç D√©tail des voitures du Cluster 4 (les 6 premi√®res) :")
    
    # S√©curit√© : on v√©rifie si des voitures existent dans ce cluster
    if not cluster_4_cars.empty:
        cols_affichage = ["marque", "modele", "prix", "kilometrage", "annee", "puissance_kw"]
        # On ne garde que les colonnes pr√©sentes pour √©viter une nouvelle KeyError
        cols_presentes = [c for c in cols_affichage if c in cluster_4_cars.columns]
        print(cluster_4_cars[cols_presentes].head(6))
    else:
        print("Aucun v√©hicule trouv√© dans le cluster 4.")
    
    print("‚úÖ Clustering termin√© et pipeline sauvegard√©.")
    return df

# =========================
# 2. ANALYSE ET VISUALISATION
# =========================
def analyser_clusters(df):
    """
    G√©n√®re un profil complet des segments de march√© identifi√©s.
    
    Cette fonction r√©alise une analyse √† trois niveaux :
    1. Statistique : Calcule les m√©dianes par cluster pour identifier les segments 
       (ex: cluster "Haut de gamme", cluster "Petit budget/Fort kilom√©trage").
    2. Distribution : G√©n√®re des boxplots pour visualiser la dispersion et les 
       valeurs aberrantes au sein de chaque groupe.
    3. Structurelle : Appelle la PCA pour valider visuellement la coh√©rence du clustering.

    Args:
        df (pl.DataFrame|pd.DataFrame): Donn√©es incluant la colonne 'cluster_vehicule'.
    """
    # Conversion pour l'analyse
    data_pd = df.to_pandas() if isinstance(df, pl.DataFrame) else df

    if "cluster_vehicule" not in data_pd.columns:
        print("‚ö†Ô∏è Erreur : Colonne 'cluster_vehicule' manquante pour l'analyse.")
        return
    
    # 1. Statistiques descriptives
    stats_config = {
        "prix": ["median", "mean"],
        "kilometrage": "median",
        "annee": "median",
        "puissance_kw": "median",
        "cylindree_l": "median"
    }
    
    # On ne garde que les cl√©s pr√©sentes dans le DataFrame
    agg_dict = {k: v for k, v in stats_config.items() if k in data_pd.columns}

    print("\nüß† Profil statistique des clusters :")
    if agg_dict:
        resume = data_pd.groupby("cluster_vehicule").agg(agg_dict).round(0)
        print(resume)
    else:
        print("Aucune colonne num√©rique trouv√©e pour les statistiques.")

    # 2. Visualisation (Boxplots) dynamique
    cols_a_voir = [c for c in ["prix", "kilometrage", "annee", "puissance_kw"] if c in data_pd.columns]
    
    if cols_a_voir:
        # Calcul du nombre de lignes/colonnes pour les subplots
        n_cols = 2
        n_rows = (len(cols_a_voir) + 1) // 2
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
        axes = axes.flatten()

        for i, col in enumerate(cols_a_voir):
            sns.boxplot(x="cluster_vehicule", y=col, data=data_pd, ax=axes[i], 
                        hue="cluster_vehicule", palette="Set2", legend=False)
            axes[i].set_title(f"Distribution de {col}")
        
        # Supprimer les axes vides si nombre impair de graphiques
        for j in range(i + 1, len(axes)):
            fig.delaxes(axes[j])
            
        plt.tight_layout()
        plt.show()

    # 3. PCA (uniquement si on a assez de colonnes num√©riques)
    if len(data_pd.select_dtypes(include=[np.number]).columns) >= 3:
        visualiser_pca(data_pd)

def visualiser_pca(df_pd):
    """
    Projette le dataset multidimensionnel en 2D pour valider la s√©paration des clusters.
    
    Logique technique :
    - R√©cup√®re le pipeline d'origine pour appliquer exactement le m√™me pr√©traitement 
      (Imputation + Scaling) que lors de l'entra√Ænement.
    - Utilise l'Analyse en Composantes Principales (PCA) pour r√©duire les features 
      (prix, puissance, ann√©e...) en deux axes synth√©tiques.
    - Permet de d√©tecter visuellement si les clusters se chevauchent ou s'ils 
      sont bien distincts dans l'espace latent.

    Args:
        df_pd (pd.DataFrame): Donn√©es au format Pandas pour compatibilit√© Scikit-Learn.
    """
    try:
        # On recharge le pipeline pour transformer les donn√©es
        with open("models/cluster_full_pipeline.pkl", "rb") as f:
            pipeline = pickle.load(f)
        
        # Transformation des donn√©es par le preprocessor du pipeline
        X_transformed = pipeline.named_steps['preprocessor'].transform(df_pd)
        
        pca = PCA(n_components=2)
        coords = pca.fit_transform(X_transformed)
        
        plt.figure(figsize=(10, 6))
        sns.scatterplot(x=coords[:, 0], y=coords[:, 1], hue=df_pd["cluster_vehicule"], palette="deep", alpha=0.5)
        plt.title("S√©paration des clusters (Projection PCA)")
        plt.show()
    except Exception as e:
        print(f"Impossible de g√©n√©rer la PCA : {e}")

def trouver_meilleur_k(df, max_k=10):
    """
    Optimise le param√®tre K (nombre de clusters) via les m√©thodes du Coude et de la Silhouette.

    Cette fonction aide √† d√©terminer le partitionnement le plus naturel des donn√©es en 
    analysant deux m√©triques compl√©mentaires sur une plage de valeurs de K :

    1. M√©thode du Coude (Elbow) :
       - Mesure l'inertie intra-classe (somme des carr√©s des distances au centro√Øde).
       - Objectif : Identifier le point d'inflexion ("le coude") o√π l'ajout d'un 
         cluster suppl√©mentaire ne r√©duit plus l'inertie de mani√®re significative.

    2. Score de Silhouette :
       - Mesure √† quel point un objet est similaire √† son propre cluster par rapport 
         aux autres clusters (entre -1 et 1).
       - Objectif : Un score √©lev√© indique que les v√©hicules sont bien class√©s dans 
         leur groupe et loin des groupes voisins.

    Parameters:
    -----------
    df : polars.DataFrame or pandas.DataFrame
        Le dataset contenant les caract√©ristiques techniques des v√©hicules.
    max_k : int, default=10
        Le nombre maximum de clusters √† tester.

    Workflow :
    ----------
    - Extraction automatique des features num√©riques disponibles (prix, km, etc.).
    - Standardisation des donn√©es pour assurer une contribution √©quitable de chaque variable.
    - G√©n√©ration de graphiques d√©cisionnels pour guider le choix de l'utilisateur.

    Returns:
    --------
    tuple (list, list) :
        Une liste des inerties et une liste des scores de silhouette pour chaque K test√©.
    """
    data_pd = df.to_pandas() if isinstance(df, pl.DataFrame) else df.copy()
    
    # Liste des features id√©ales
    cibles = ["annee", "kilometrage", "puissance_kw", "prix", "cylindree_l"]
    # On ne garde que celles qui existent vraiment dans le DF
    num_features = [c for c in cibles if c in data_pd.columns]
    
    if len(num_features) < 2:
        print("‚ö†Ô∏è Pas assez de colonnes num√©riques pour le clustering.")
        return [], []

    # Pr√©traitement : on drop les lignes avec des NaN uniquement sur ces colonnes
    data_clean = data_pd[num_features].dropna()
    
    if len(data_clean) < max_k:
        print(f"‚ö†Ô∏è Trop peu de donn√©es ({len(data_clean)}) pour max_k={max_k}.")
        max_k = max(2, len(data_clean) - 1)

    X = StandardScaler().fit_transform(data_clean)
    
    # 1. Calcul des inerties (Elbow)
    inertias = []
    ks_elbow = range(1, max_k + 1)
    for k in ks_elbow:
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        km.fit(X)
        inertias.append(km.inertia_)
    
    plt.figure(figsize=(8, 4))
    plt.plot(ks_elbow, inertias, 'go-')
    plt.title("M√©thode du Coude (Elbow)")
    plt.xlabel("Nombre de clusters")
    plt.ylabel("Inertie")
    plt.show()
    
    # 2. Calcul des silhouettes
    silhouettes = []
    ks_sil = range(2, max_k + 1)
    for k in ks_sil:
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        km.fit(X)
        silhouettes.append(silhouette_score(X, km.labels_))
    
    plt.figure(figsize=(8, 4))
    plt.plot(ks_sil, silhouettes, 'bo-')
    plt.title("Score de Silhouette")
    plt.xlabel("Nombre de clusters")
    plt.ylabel("Score")
    plt.show()

    return inertias, silhouettes

# =========================
# CHARGEMENT & PR√âPARATION
# =========================
def charger_et_preparer_donnees(fichier="data/processed/autoscout_clean_ml.json"):
    """
    Transforme les donn√©es nettoy√©es en matrices de features (X) et cible (y) pour le ML.

    Cette fonction orchestre le passage des donn√©es brutes vers un format num√©rique :
    1. Enrichissement : Int√®gre le clustering comme une nouvelle feature pr√©dictive.
    2. Filtrage : √âcarte les variables √† haute cardinalit√© (ville, CP) qui pourraient 
       causer du surapprentissage (overfitting).
    3. Encodage (Dummy Coding) : Convertit les variables textuelles en colonnes binaires.
    4. Alignement : Garantit que l'ensemble de test poss√®de exactement les m√™mes 
       colonnes que l'ensemble d'entra√Ænement, m√™me si certaines cat√©gories y sont absentes.

    Args:
        fichier (str): Chemin vers le fichier JSON trait√©.

    Returns:
        tuple: (X_train, X_test, y_train, y_test) sous forme de DataFrames/Series Pandas.
    """
    try:
        df = pl.read_json(fichier).to_pandas()
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement : {e}")
        return None, None, None, None

    if "modele_identifie" in df.columns: #modele_identifi√© supp dans cleaning
        df = df[df["modele_identifie"]]

    df = df.dropna(subset=["prix"]) #pour √™tre s√ªr

    # üîπ 1. Clustering
    df = ajouter_cluster_vehicule(df, n_clusters=5)
    
    # üîπ 2. AJOUT ICI : Conversion en string pour que get_dummies le traite en cat√©gories
    df["cluster_vehicule"] = df["cluster_vehicule"].astype(str) 
    
    analyser_clusters(df)

    # üîπ 3. Pr√©paration des variables
    y = df["prix"]

    colonnes_a_exclure = [
        "prix",
        "code_postal",
        "ville",
    ]
    X = df.drop(columns=[c for c in colonnes_a_exclure if c in df.columns])

    categorical_cols = X.select_dtypes(include=["object", "category", "string"]).columns.tolist()
    print(f"üì¶ Colonnes encod√©es : {categorical_cols}")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )#test_size a verif pour que ce soit l'input de l'app (0.2 par defaut)

    X_train = pd.get_dummies(X_train, columns=categorical_cols)
    X_test = pd.get_dummies(X_test, columns=categorical_cols)

    X_train, X_test = X_train.align(X_test, join="left", axis=1, fill_value=0) # √† v√©rifier

    # V√©rification de s√©curit√©
    if not any("cluster_vehicule" in col for col in X_train.columns):
        raise ValueError("üö® Erreur critique : la colonne cluster_vehicule a disparu lors de l'encodage !")
    
    # Cherche n'importe quelle colonne qui commence par "cluster_vehicule"
    assert any("cluster_vehicule" in col for col in X_train.columns)

    return X_train, X_test, y_train, y_test


# =========================
# √âVALUATION
# =========================
def evaluer_modele(model, X_test, y_test):
    """
    √âvalue la performance du mod√®le de r√©gression sur l'ensemble de test.

    Cette fonction calcule et affiche les trois m√©triques fondamentales pour la 
    pr√©diction de prix :
    1. RMSE (Root Mean Square Error)
    2. R¬≤ (Coefficient de d√©termination)
    3. MAE (Mean Absolute Error)

    Args:
        model: Le mod√®le entra√Æn√© (RandomForest, XGBoost, etc.).
        X_test (pd.DataFrame): Les caract√©ristiques de l'ensemble de test.
        y_test (pd.Series): Les prix r√©els correspondants.

    Returns:
        tuple: (y_pred, rmse, r2, mae) pour analyse ult√©rieure ou visualisation.
    """
    y_pred = model.predict(X_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))
    r2_test = r2_score(y_test, y_pred)
    mae_test = mean_absolute_error(y_test, y_pred)
    print(f"RMSE: ‚Ç¨{rmse_test:,.2f} | R¬≤: {r2_test:.4f} | MAE: ‚Ç¨{mae_test:,.2f}")
    return y_pred, rmse_test, r2_test, mae_test


def cross_validation_model(model, X_train, y_train, cv=5):
    """
    √âvalue la stabilit√© et la capacit√© de g√©n√©ralisation du mod√®le via une validation crois√©e.

    Args:
        model: L'estimateur scikit-learn √† √©valuer.
        X_train (pd.DataFrame): Matrice des caract√©ristiques d'entra√Ænement.
        y_train (pd.Series): Vecteur de la variable cible (prix).
        cv (int): Nombre de segments (folds). Par d√©faut 5.

    Returns:
        np.array: Liste des scores R¬≤ obtenus pour chaque segment.
    """
    scores = cross_val_score(model, X_train, y_train, scoring="r2", cv=cv, n_jobs=-1)
    print(f"üîÅ CV R¬≤: {scores.mean():.4f} ¬± {scores.std():.4f}")
    return scores


def enregistrer_erreurs(X_test, y_test, y_pred, fichier):
    """
    G√©n√®re un rapport d√©taill√© des erreurs de pr√©diction pour analyse post-mortem.

    Cette fonction cr√©e un fichier (Excel ou CSV) permettant d'identifier les cas 
    sp√©cifiques o√π le mod√®le √©choue. 

    Args:
        X_test (pd.DataFrame): Caract√©ristiques des v√©hicules de test.
        y_test (pd.Series): Prix r√©els.
        y_pred (np.array): Prix pr√©dits par le mod√®le.
        fichier (str): Chemin de destination (ex: 'data/errors/debug_cars.xlsx').
    """
    # 1. On ne garde que les colonnes num√©riques "r√©elles" pour que l'Excel soit lisible
    # On exclut les colonnes de type dummies
    cols_lisibles = [c for c in X_test.columns if '_' not in c]
    df_err = X_test[cols_lisibles].copy()

    # 2. Calculs des erreurs
    df_err["prix_reel"] = y_test.values
    df_err["prix_predit"] = np.round(y_pred, 2)
    df_err["erreur_abs"] = np.abs(df_err["prix_predit"] - df_err["prix_reel"])
    df_err["erreur_%"] = (df_err["erreur_abs"] / df_err["prix_reel"] * 100).round(2)

    # 3. Ajout d'un diagnostic m√©tier
    df_err["diagnostic"] = np.where(
        df_err["prix_predit"] > df_err["prix_reel"], 
        "Sur-estim√©", 
        "Sous-estim√©"
    )
    
    # 4. Tri par les plus grosses erreurs en pourcentage
    df_err = df_err.sort_values("erreur_%", ascending=False)
    
    # 5. Sauvegarde
    if fichier.endswith('.xlsx'):
        df_err.to_excel(fichier, index=False)
    else:
        df_err.to_csv(fichier, index=False)
        
    print(f"üíæ Fichier d'erreurs enregistr√© : {fichier} ({len(df_err)} lignes)")

# =========================
# MOD√àLES
# =========================
def tune_random_forest(X_train, y_train):
    """
    Optimise les r√©glages du RandomForest via une recherche al√©atoire (Randomized Search).

    Au lieu de tester toutes les combinaisons possibles (GridSearch), cette fonction 
    explore intelligemment l'espace des hyperparam√®tres pour trouver le meilleur 
    compromis entre pr√©cision et temps de calcul.

    Param√®tres cl√©s optimis√©s :
    -------------------------
    - n_estimators
    - max_depth
    - min_samples_split
    - min_samples_leaf
    - max_features

    Args:
        X_train (pd.DataFrame): Donn√©es d'entra√Ænement encod√©es.
        y_train (pd.Series): Prix cibles.

    Returns:
        model: Le meilleur estimateur RandomForest trouv√© lors de la recherche.
    """
    print("\nüîç Tuning Hyperparam√®tres : RANDOM FOREST")
    
    param_dist = {
        'n_estimators': [100, 300, 500],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2', None]
    }
    
    rf = RandomForestRegressor(random_state=42, n_jobs=-1)
    
    # n_iter=10 teste 10 combinaisons au hasard
    search = RandomizedSearchCV(
        rf, param_distributions=param_dist, 
        n_iter=10, cv=5, scoring='r2', verbose=1, random_state=42, n_jobs=-1
    )
    #scoring( 'r2' a passer en input metrique app) voir si il existe d'autre fa√ßons de scoring
    search.fit(X_train, y_train)
    print(f"‚úÖ Meilleurs param√®tres RF: {search.best_params_}")
    return search.best_estimator_

def entrainer_random_forest(model_tune, X_train, X_test, y_train, y_test):
    """
    Finalise l'entra√Ænement du mod√®le Random Forest et archive les r√©sultats.

    Cette fonction prend le meilleur estimateur issu du tuning et r√©alise 
    un cycle complet de validation pour garantir la fiabilit√© des pr√©dictions.

    Points cl√©s du workflow :
    ------------------------
    1. Comparaison Train vs Test : Calcule le R¬≤ sur les deux sets pour d√©tecter 
       un √©ventuel surapprentissage (si R¬≤ Train >>> R¬≤ Test).
    2. Validation Crois√©e : Confirme la stabilit√© du mod√®le sur 5 d√©coupages diff√©rents.
    3. Analyse d'Importance : Identifie les variables qui influencent le plus le prix 
       (ex: l'ann√©e vs le kilom√©trage).
    4. Persistance : Sauvegarde le mod√®le au format .pkl pour l'application web.
    5. Debugging : G√©n√®re un rapport d'erreurs Excel pour l'analyse humaine.

    Args:
        model_tune: Le mod√®le RandomForest avec ses hyperparam√®tres d√©j√† optimis√©s.
        X_train, X_test: Matrices de caract√©ristiques (features).
        y_train, y_test: Vecteurs cibles (prix).

    Returns:
        dict: Un dictionnaire complet contenant le mod√®le, les m√©triques et l'importance des variables.
    """
    print("\nüå≤ RANDOM FOREST")
    model = model_tune
    model.fit(X_train, y_train) #Optionnel
    
    # 1. Score R¬≤ sur les donn√©es d'entra√Ænement (Train Score)
    r2_train = model.score(X_train, y_train)
    print(f"üìä R¬≤ sur donn√©es Train : {r2_train:.4f}")
    
    # 2. √âvaluation sur test
    y_pred, rmse_test, r2_test, mae_test = evaluer_modele(model, X_test, y_test)
    
    # 3. Cross-Validation (on r√©cup√®re la moyenne des scores R¬≤)
    print("üîÑ Calcul de la Cross-Validation (R¬≤)...")
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    r2_cv = cv_scores.mean()
    print(f"üéØ R¬≤ Moyen (Cross-Validation) : {r2_cv:.4f}")

    # Importance des variables
    fi = pd.DataFrame({
        "feature": X_train.columns,
        "importance": model.feature_importances_
    }).sort_values("importance", ascending=False)

    print(fi.head(10))
    print(fi[fi["feature"] == "cluster_vehicule"])

    # Sauvegarde
    with open("models/random_forest_model.pkl", "wb") as f:
        pickle.dump(model, f)

    enregistrer_erreurs(X_test, y_test, y_pred, "models/erreurs_rf.xlsx")
    
    return {
        'model': model,
        'rmse': rmse_test,
        'r2': r2_cv,
        'r2_train': r2_train,
        'r2_test': r2_test,
        'mae': mae_test,
        'feature_importance': fi
    }

def tune_xgboost(X_train, y_train):
    """
    Optimise les hyperparam√®tres du mod√®le XGBoost via une recherche al√©atoire.

    Le XGBoost est un algorithme puissant mais sensible au r√©glage de ses param√®tres. 
    Cette fonction cherche l'√©quilibre optimal entre vitesse d'apprentissage et 
    capacit√© de g√©n√©ralisation.

    Param√®tres cl√©s optimis√©s :
    -------------------------
    - n_estimators
    - max_depth
    - learning_rate
    - subsample
    - colsample_bytree
    - gamma

    Args:
        X_train (pd.DataFrame): Donn√©es d'entra√Ænement encod√©es.
        y_train (pd.Series): Prix cibles.

    Returns:
        model: Le meilleur estimateur XGBRegressor trouv√©.
    """
    print("\nüîç Tuning Hyperparam√®tres : XGBOOST")
    
    param_dist = {
        'n_estimators': [200, 500, 1000],
        'max_depth': [3, 6, 9],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.7, 0.8, 0.9],
        'colsample_bytree': [0.7, 0.8, 0.9],
        'gamma': [0, 0.1, 0.2]
    }
    
    xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1, verbosity=0)
    
    search = RandomizedSearchCV(
        xgb_model, param_distributions=param_dist, 
        n_iter=10, cv=5, scoring='r2', verbose=1, random_state=42, n_jobs=-1
    ) #scoring comme pour RF passer a l'imput app
    
    search.fit(X_train, y_train)
    print(f"‚úÖ Meilleurs param√®tres XGB: {search.best_params_}")
    return search.best_estimator_

def entrainer_xgboost(model_tune,X_train, X_test, y_train, y_test):
    """
    Ex√©cute l'entra√Ænement final du mod√®le XGBoost et g√©n√®re le bilan de performance.

    Points d'attention :
    ------------------
    1. Robustesse (R¬≤ CV)
    2. Overfitting : La comparaison entre r2_train et r2_cv 
    3. Explicabilit√© : Le calcul des 'feature_importances_' permet de justifier le prix 
       pr√©dit
    4. Persistance : Le mod√®le est export√© en .pkl pour √™tre charg√© instantan√©ment 
       par ton application de pr√©diction.

    Args:
        model_tune: Le mod√®le XGBRegressor optimis√© par RandomizedSearchCV.
        X_train, X_test: Ensembles de caract√©ristiques.
        y_train, y_test: Ensembles de prix cibles.

    Returns:
        dict: Dictionnaire complet des r√©sultats (mod√®le, m√©triques d'erreur et importances).
    """
    print("\n‚ö° XGBOOST")
    model = model_tune
    
    model.fit(X_train, y_train) # Optionnel
    
    # 1. Score R¬≤ sur les donn√©es d'entra√Ænement
    r2_train = model.score(X_train, y_train)
    print(f"üìä R¬≤ sur donn√©es Train : {r2_train:.4f}")
    
    # 2. √âvaluation sur test
    y_pred, rmse_test, r2_test, mae_test = evaluer_modele(model, X_test, y_test)
    
    # 3. Cross-Validation
    print("üîÑ Calcul de la Cross-Validation (R¬≤)...")
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    r2_cv = cv_scores.mean()
    print(f"üéØ R¬≤ Moyen (Cross-Validation) : {r2_cv:.4f}")
    
    # Importance des variables
    fi = pd.DataFrame({
        "feature": X_train.columns,
        "importance": model.feature_importances_
    }).sort_values("importance", ascending=False)

    print(fi.head(10))
    print(fi[fi["feature"] == "cluster_vehicule"])
    
    # Sauvegarde
    with open("models/xgboost_model.pkl", "wb") as f:
        pickle.dump(model, f)
    
    enregistrer_erreurs(X_test, y_test, y_pred, "models/erreurs_xgb.xlsx")

    return {
        'model': model,
        'rmse': rmse_test,
        'r2': r2_cv, #a verif prendre le R2 de cross validation et pas celui d'evaluation
        'r2_train': r2_train,
        'r2_test': r2_test,
        'mae': mae_test,
        'feature_importance': fi
    }

# =========================
# MAIN
# =========================
def main():
    """
    Chef d'orchestre du pipeline de Machine Learning AutoScout24.
    
    Cette fonction automatise le cycle de vie complet du mod√®le :
    1. Infrastructure : Cr√©e les dossiers n√©cessaires et initialise le logging.
    2. Data : Charge, nettoie, clust√©rise et pr√©pare les matrices X/y.
    3. Persistance des Features : Sauvegarde la liste exacte des colonnes pour 
       garantir que l'application de pr√©diction (Streamlit) utilise le m√™me format.
    4. Optimisation (Tuning) : Si aucun mod√®le n'est d√©tect√©, lance une recherche 
       d'hyperparam√®tres (RandomizedSearchCV) pour RF et XGBoost.
    5. Analyse : G√©n√®re des visualisations d'importance des variables pour 
       comprendre les leviers du prix (Ann√©e, Puissance, Clusters).
    6. Benchmarking : Compare les performances (R¬≤, RMSE, MAE) entre les deux 
       algorithmes et d√©signe le vainqueur.
    7. Audit : Exporte les erreurs de pr√©diction pour le d√©bogage m√©tier.
    """
    # Configuration initiale
    os.makedirs("models", exist_ok=True)
    print("\n" + "="*50)
    print("üöÄ D√âMARRAGE DU PIPELINE MACHINE LEARNING")
    print("="*50)

    fichier_input = "data/processed/autoscout_clean_ml.json"
    rf_path = "models/best_rf_final.pkl"
    xgb_path = "models/best_xgb_final.pkl"
    features_path = "models/model_features.pkl"
    
    # 1. PR√âPARATION DES DONN√âES & CLUSTERING
    print("\nüì¶ 1. Chargement et Pr√©paration des donn√©es...")
    X_train, X_test, y_train, y_test = charger_et_preparer_donnees(fichier_input)
    
    if X_train is None:
        print("‚ùå √âchec du chargement. Fin du programme.")
        return
    
    # Sauvegarde des colonnes pour l'application Streamlit
    with open(features_path, "wb") as f:
        pickle.dump(X_train.columns.tolist(), f)
        
    print(f"‚úÖ Dataset pr√™t : {X_train.shape[0]} train / {X_test.shape[0]} test")
    print(f"üìä Nombre de features (colonnes) : {len(X_train.columns)}")
    
    # 2. CHARGEMENT OU TUNING (A revoir car conflits OneHotEncoder et get dummies)
    if os.path.exists(rf_path) and os.path.exists(xgb_path):
        print("\n‚ôªÔ∏è  2. Mod√®les optimis√©s trouv√©s. Chargement en cours...")
        with open(rf_path, "rb") as f:
            best_rf_model = pickle.load(f)
        with open(xgb_path, "rb") as f:
            best_xgb_model = pickle.load(f)
        
        # S√©curit√© : Alignement des donn√©es sur les mod√®les charg√©s
        expected_features = best_rf_model.feature_names_in_ if hasattr(best_rf_model, "feature_names_in_") else X_train.columns.tolist()
        X_train = X_train.reindex(columns=expected_features, fill_value=0)
        X_test = X_test.reindex(columns=expected_features, fill_value=0)
    else:
        print("\nüîç 2. Aucun mod√®le trouv√©. Lancement de l'optimisation (Tuning)...")
        # On utilise tes fonctions de tuning qui font le RandomizedSearchCV
        best_rf_model = tune_random_forest(X_train, y_train)
        best_xgb_model = tune_xgboost(X_train, y_train)
        
        # Sauvegarde des mod√®les optimis√©s
        with open(rf_path, "wb") as f:
            pickle.dump(best_rf_model, f)
        with open(xgb_path, "wb") as f:
            pickle.dump(best_xgb_model, f)
        print("‚úÖ Tuning termin√© et mod√®les sauvegard√©s.")

    # 3. ANALYSE DES VARIABLES (Feature Importance)
    print("\nüìä 3. Analyse des variables d'influence...")
    
    # --- GRAPHIQUE 1 : RANDOM FOREST ---
    fi_rf = pd.DataFrame({
        "feature": X_train.columns,
        "importance": best_rf_model.feature_importances_
    }).sort_values("importance", ascending=False).head(15)
    
    plt.figure(figsize=(10, 8))
    sns.barplot(
        x="importance", 
        y="feature", 
        data=fi_rf, 
        hue="feature", 
        palette="magma", 
        legend=False
    )
    plt.title("Top 15 Features - Random Forest")
    plt.tight_layout()
    plt.show()  # Bloque ici jusqu'√† ce que tu fermes la fen√™tre

    # --- GRAPHIQUE 2 : XGBOOST ---
    xgb_importances = best_xgb_model.get_booster().get_score(importance_type='gain')
    fi_xgb = pd.DataFrame({
        "feature": list(xgb_importances.keys()),
        "importance": list(xgb_importances.values())
    }).sort_values("importance", ascending=False).head(15)
    
    plt.figure(figsize=(10, 8))
    sns.barplot(
        x="importance", 
        y="feature", 
        data=fi_xgb, 
        hue="feature", 
        palette="viridis", 
        legend=False
    )
    plt.title("Top 15 Features - XGBoost (Importance par Gain)")
    plt.xlabel("Gain moyen apport√© par la variable")
    plt.ylabel("Variables")
    plt.tight_layout()
    plt.show()

    # 4. √âVALUATION FINALE & COMPARAISON
    print("\nüèÜ 4. √âVALUATION D√âTAILL√âE DES MOD√àLES")
    
    # --- RANDOM FOREST ---
    print("\n" + "="*40)
    print("üå≤ MOD√àLE : RANDOM FOREST")
    print("="*40)
    # Cette fonction affiche d√©j√† le R2 Train, le R2 CV et le R2 Test
    res_rf = entrainer_random_forest(best_rf_model, X_train, X_test, y_train, y_test)
    
    # --- XGBOOST ---
    print("\n" + "="*40)
    print("‚ö° MOD√àLE : XGBOOST")
    print("="*40)
    res_xgb = entrainer_xgboost(best_xgb_model, X_train, X_test, y_train, y_test)
    
    print("\n" + "="*40)
    
    # 5. R√âSUM√â COMPARATIF FINAL
    print("\nüèÅ R√âSUM√â DES PERFORMANCES (R¬≤)")
    print(f"{'Mod√®le':<20} | {'CV (Train)':<12} | {'Test Set':<12}")
    print("-" * 50)
    print(f"{'Random Forest':<20} | {res_rf['r2']:.4f}     | {res_rf['r2_test']:.4f}")
    print(f"{'XGBoost':<20} | {res_xgb['r2']:.4f}     | {res_xgb['r2_test']:.4f}")

    # On r√©cup√®re les scores pour la conclusion finale
    r2_rf_final = res_rf['r2_test']
    r2_xgb_final = res_xgb['r2_test']
    
    # 5. ENREGISTREMENT DES ERREURS
    enregistrer_erreurs(X_test, y_test, res_rf['model'].predict(X_test), "models/erreurs_rf_tuned.xlsx")
    enregistrer_erreurs(X_test, y_test, res_xgb['model'].predict(X_test), "models/erreurs_xgb_tuned.xlsx")
    
    # Conclusion
    meilleur_modele = "XGBoost" if r2_xgb_final > r2_rf_final else "Random Forest"
    best_score = max(r2_rf_final, r2_xgb_final)
    
    print("\n" + "="*50)
    print("‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS")
    print(f"‚≠ê MEILLEUR MOD√àLE : {meilleur_modele}")
    print(f"üéØ SCORE R¬≤ FINAL : {best_score:.4f}")
    print("="*50 + "\n")

if __name__ == "__main__":
    main()
